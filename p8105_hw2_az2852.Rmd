---
title: "p8105_hw2_az2852"
output: github_document
---

```{r setup, message=FALSE}
library(tidyverse)
library(readxl)
```

# Problem 1
```{r, message=FALSE}
transit_df = 
  read_csv("./data/nyc_transit.csv", na = c("NA", ".", ""))%>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  distinct() %>% 
  mutate(across(c(route1: route11), as.character)) %>% 
  pivot_longer(
    cols = route1:route11,
    names_to = "route_count",
    names_prefix = "route",
    values_to = "subway_lines") %>% 
  drop_na("subway_lines") %>% 
  mutate(
    entry = case_match(
      entry,
      "YES"  ~ TRUE,
      "NO" ~ FALSE
    )) %>% 
  mutate(
    vending = case_match(
      vending,
      "YES"  ~ TRUE,
      "NO" ~ FALSE
    ))

```
The resulting dataset (1,566 × 10) contains information about each entrance and exit for each subway station in NYC. Important variables include line, station name, station latitude & longitude, routes served, entry, entrance type, vending, and ADA compliance. So far, I've converted all variable names to lower-case string format, removed repeated rows, converted the original "route" variables to long format under a new variable called "subway_lines"; the number of subway lines running at each station is displayed under a variable called "route_count". I removed the NAs in the subway_lines column. Finally, I converted the entry and vending variables to logical.

Now the data is tidy in the sense that you can clearly see which subway lines are running at each station. There are no repeating rows and the display is better. Converting the entry variable to logical allows us to count the number of available entrances and see which stations have available entrances.

According to the dataset:

* There are `r transit_df %>% distinct(line, station_name) %>% nrow()` distinct stations.
* `r transit_df %>% distinct(line, station_name, .keep_all = TRUE) %>% pull(ada) %>% sum()` stations are ADA compliant.
* About 55% of stations without vending allow entrance.

The code for calculation are as follows:
```{r}
transit_df %>% 
  distinct(line, station_name) %>% 
  nrow()
```
```{r}
transit_df %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  pull(ada) %>% 
  sum()
```
```{r}
no_vending_df = transit_df %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(vending == FALSE)

no_vending_entry_df = no_vending_df %>% 
  filter(entry == TRUE)

nrow(no_vending_entry_df)/nrow(no_vending_df)
```

## How many distinct stations serve the A train? 
```{r}
transit_df %>% 
  filter(subway_lines == "A") %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  nrow()
```
60 distinct stations serve the A train.

## Of the stations that serve the A train, how many are ADA compliant?
```{r}
transit_df %>% 
  filter(subway_lines == "A") %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(ada == TRUE) %>% 
  nrow()
```
Of the stations that serve the A train, 17 are ADA compliant.


# Problem 2
Read the Mr. Trashwheel dataset.
```{r}
mr_trashwheel_df = 
    read_xlsx(
        path = "./data/trash_wheel.xlsx",
        sheet = "Mr. Trash Wheel",
        range = cell_cols("A:N")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster) %>% 
    mutate(
        sports_balls = round(sports_balls),
        sports_balls = as.integer(sports_balls),
        year = as.numeric(year)
    ) 

```
Read Professor Trash Wheel dataset.
```{r}
prof_trashwheel_df = 
    read_xlsx(
        path = "./data/trash_wheel.xlsx",
        sheet = "Professor Trash Wheel",
        range = cell_cols("A:M")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster)
```

Read Gwynda Trash Wheel dataset.
```{r}
gw_trashwheel_df = 
    read_xlsx(
        path = "./data/trash_wheel.xlsx",
        sheet = "Gwynnda Trash Wheel",
        range = cell_cols("A:L")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster)
```
Now we try to combine the 3 datasets together. We observe that the 3 datasets have the same variables except that Prof Trashwheel lacks "sports_balls" and Gwynnda Trashwheel lacks "glass_bottles" and "sports_balls". We need to add them before combining.
```{r}
prof_trashwheel_tidy_df = 
  prof_trashwheel_df %>% 
  mutate(sports_balls = NA) %>% 
  select(1:which(names(prof_trashwheel_df) == "wrappers"), sports_balls, everything()) 

```

```{r}
gw_trashwheel_tidy_df = 
  gw_trashwheel_df %>% 
  mutate(glass_bottles = NA, sports_balls = NA) %>% 
  select(1:which(names(gw_trashwheel_df) == "wrappers"), sports_balls, everything()) %>% 
  select(1:which(names(gw_trashwheel_df) == "cigarette_butts"), glass_bottles, everything())
 
```


Now combine the datasets.
```{r}
all_trashwheels_df = bind_rows(mr_trashwheel_df, prof_trashwheel_tidy_df, gw_trashwheel_tidy_df)
```
This dataset (845 x 14) contains information from the Mr. Trashwheel trash collectors in Baltimore, Maryland. As trash enters the inner harbor, the trashwheels collect that trash, and store it in a dumpster. The dataset contains information on year, month, and trash collected, including specific kinds of trash. There are a total of 845 rows in our final dataset.

* The total weight of trash collected by Professor Trash Wheel is 216.26 tons.
* The total number of cigarette butts collected by Gwynnda in June of 2022 is 18120.
```{r}
sum(pull(prof_trashwheel_tidy_df, weight_tons))

gw_trashwheel_tidy_df %>% 
  filter(year == 2022, month == "June") %>% 
  pull(cigarette_butts) %>% 
  sum()
```

# Problem 3
Import and clean the bakers dataset
```{r}
bakers_df = 
  read_csv("./data/gbb/bakers.csv") %>% 
  janitor::clean_names() %>% 
  arrange(series) %>% 
  rename(baker = baker_name)
```
Import and clean the bakes dataset
```{r}
bakes_df = 
  read_csv("./data/gbb/bakes.csv", na = c("N/A", "UNKNOWN", "")) %>% 
  janitor::clean_names()
```
Import and clean the results dataset.
```{r}
results_df = 
  read_csv("./data/gbb/results.csv", skip = 2) %>% 
  janitor::clean_names()
```
Now we want to create a big dataframe with all variables in one row: series, episode, baker_name, baker_age, baker_occupation, hometown, signature_bake, show_stopper, technical, result.

For the purpose of joining the datasets, I kept the first name of the bakers. I joined the bakes dataset with the results dataset, using variables "baker" "episode", and "series" as the key and keeping the observations in the results dataset. Then, I joined the dataset with the baker dataset. The bakes dataset only contains information up to the 8th season, so the bakes columns for the last 2 seasons are filled with NAs.
```{r}
bakers_fn_df = bakers_df %>% 
  mutate(baker = word(baker, 1))
```
```{r}
gbb_df =
  bakes_df %>% 
  right_join(results_df, by = c("baker", "episode", "series")) %>% 
  left_join(bakers_fn_df, by = c("baker", "series")) %>% 
  write_csv("./data/gbb/gbb.csv")
```
The dataset (1136x10) contains data from the 1st to 10th season of the Great British Bakeoff. In each episode, contestants compete in signature challenges, technical challenges, and a showstopper. At the end of an episode the winner is crowned “Star Baker” (and winner in the last episode of a season), and a loser is eliminated. The dataset contains information about the contestants, their signature bakes and showstoppers, and the results. Each row is information about one contestant in one episode.

## Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10. Comment on this table – were there any predictable overall winners? Any surprises?
```{r}
winner5t10_df = 
  gbb_df %>% 
  filter(series %in% c(5,6,7,8,9,10)) %>% 
  filter(result %in% c("WINNER", "STAR BAKER")) %>% 
  select(-signature_bake, -show_stopper) %>% 
  write_csv("./data/gbb/winner5t10.csv")
```
If we are trying to predict the overall winner based on the star bakers of each episode, we are out of luck. Surprisingly, the star baker in most episodes tends not to be the overall winner of the season. The winner tends to come from someone who is only the star baker in one or two episodes. In the most extreme case, like in season 10, the winner is someone who is never crowned star baker. In some seasons, the winner tends to be the star baker of the last few episodes.

## Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. What was the average viewership in Season 1? In Season 5?
```{r}
viewers_df =
  read_csv("./data/gbb/viewers.csv", na = "NA") %>% 
  janitor::clean_names()
```
* Mean viewership of season 1 is: 2.77
* Mean viewership of season 5 is: 10.0393
```{r}
select(viewers_df, series_1) %>% 
  drop_na() %>% 
  pull() %>% 
  mean()
```
```{r}
select(viewers_df, series_5) %>% 
  drop_na() %>% 
  pull() %>% 
  mean()
```


