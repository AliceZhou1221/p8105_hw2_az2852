p8105_hw2_az2852
================

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
```

# Problem 1

``` r
transit_df = 
  read_csv("./data/nyc_transit.csv", na = c("NA", ".", ""))%>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  distinct() %>% 
  mutate(across(c(route1: route11), as.character)) %>% 
  pivot_longer(
    cols = route1:route11,
    names_to = "route_count",
    names_prefix = "route",
    values_to = "subway_lines") %>% 
  drop_na("subway_lines") %>% 
  mutate(
    entry = case_match(
      entry,
      "YES"  ~ TRUE,
      "NO" ~ FALSE
    )) %>% 
  mutate(
    vending = case_match(
      vending,
      "YES"  ~ TRUE,
      "NO" ~ FALSE
    ))
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
print(transit_df, n = 12)
```

    ## # A tibble: 1,566 × 10
    ##    line     station_name station_latitude station_longitude entrance_type entry
    ##    <chr>    <chr>                   <dbl>             <dbl> <chr>         <lgl>
    ##  1 4 Avenue 25th St                  40.7             -74.0 Stair         TRUE 
    ##  2 4 Avenue 36th St                  40.7             -74.0 Stair         TRUE 
    ##  3 4 Avenue 36th St                  40.7             -74.0 Stair         TRUE 
    ##  4 4 Avenue 45th St                  40.6             -74.0 Stair         TRUE 
    ##  5 4 Avenue 53rd St                  40.6             -74.0 Stair         TRUE 
    ##  6 4 Avenue 53rd St                  40.6             -74.0 Stair         FALSE
    ##  7 4 Avenue 59th St                  40.6             -74.0 Stair         TRUE 
    ##  8 4 Avenue 59th St                  40.6             -74.0 Stair         TRUE 
    ##  9 4 Avenue 77th St                  40.6             -74.0 Stair         TRUE 
    ## 10 4 Avenue 77th St                  40.6             -74.0 Stair         FALSE
    ## 11 4 Avenue 86th St                  40.6             -74.0 Stair         TRUE 
    ## 12 4 Avenue 95th St                  40.6             -74.0 Stair         TRUE 
    ## # ℹ 1,554 more rows
    ## # ℹ 4 more variables: vending <lgl>, ada <lgl>, route_count <chr>,
    ## #   subway_lines <chr>

The nyc transit dataset (1,566 × 10) contains information about each
entrance and exit for each subway station in NYC. Important variables
include line, station name, station latitude & longitude, routes served,
entry, entrance type, vending, and ADA compliance. So far, I’ve
converted all variable names to lower-case string format, removed
repeated rows, converted the original “route” variables to the same
format and displayed them under a new variable called “subway_lines”;
the number of subway lines running at each station is displayed under a
variable called “route_count”. I removed the NAs in the subway_lines
column. Finally, I converted the entry and vending variables to logical.

Now the data is tidy in the sense that you can clearly see which subway
lines are running at each station. There are no repeating rows and the
display is better. Converting the entry variable to logical allows us to
count the number of available entrances and see which stations have
available entrances.

According to the dataset:

- There are 465 distinct stations.
- 84 stations are ADA compliant.
- About 55% of stations without vending allow entrance.

``` r
transit_df %>% 
  distinct(line, station_name) %>% 
  nrow()
```

    ## [1] 465

``` r
transit_df %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  pull(ada) %>% 
  sum()
```

    ## [1] 84

``` r
no_vending_df = transit_df %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(vending == FALSE)

no_vending_entry_df = no_vending_df %>% 
  filter(entry == TRUE)

nrow(no_vending_entry_df)/nrow(no_vending_df)
```

    ## [1] 0.5555556

# Problem 2

Read the Mr. Trashwheel dataset.

``` r
mr_trashwheel_df = 
    read_xlsx(
        path = "./data/trash_wheel.xlsx",
        sheet = "Mr. Trash Wheel",
        range = cell_cols("A:N")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster) %>% 
    mutate(
        sports_balls = round(sports_balls),
        sports_balls = as.integer(sports_balls),
        year = as.numeric(year)
    ) 
print(mr_trashwheel_df, n = 12)
```

    ## # A tibble: 584 × 14
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## 11       11 June   2014 2014-06-11 00:00:00        3.43                 15
    ## 12       12 June   2014 2014-06-12 00:00:00        4.17                 19
    ## # ℹ 572 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>

Read Professor Trash Wheel dataset.

``` r
prof_trashwheel_df = 
    read_xlsx(
        path = "./data/trash_wheel.xlsx",
        sheet = "Professor Trash Wheel",
        range = cell_cols("A:M")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster)

print(prof_trashwheel_df, n = 12)
```

    ## # A tibble: 106 × 13
    ##    dumpster month     year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>    <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 January   2017 2017-01-02 00:00:00        1.79                 15
    ##  2        2 January   2017 2017-01-30 00:00:00        1.58                 15
    ##  3        3 February  2017 2017-02-26 00:00:00        2.32                 18
    ##  4        4 February  2017 2017-02-26 00:00:00        3.72                 15
    ##  5        5 February  2017 2017-02-28 00:00:00        1.45                 15
    ##  6        6 March     2017 2017-03-30 00:00:00        1.71                 15
    ##  7        7 April     2017 2017-04-01 00:00:00        1.82                 15
    ##  8        8 April     2017 2017-04-20 00:00:00        2.37                 15
    ##  9        9 May       2017 2017-05-10 00:00:00        2.64                 15
    ## 10       10 May       2017 2017-05-26 00:00:00        2.78                 15
    ## 11       11 June      2017 2017-06-20 00:00:00        2.34                 15
    ## 12       12 July      2017 2017-07-17 00:00:00        1.63                 15
    ## # ℹ 94 more rows
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>

Read Gwynda Trash Wheel dataset.

``` r
gw_trashwheel_df = 
    read_xlsx(
        path = "./data/trash_wheel.xlsx",
        sheet = "Gwynnda Trash Wheel",
        range = cell_cols("A:L")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster)

print(gw_trashwheel_df, n = 12)
```

    ## # A tibble: 155 × 12
    ##    dumpster month   year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 July    2021 2021-07-03 00:00:00        0.93                 15
    ##  2        2 July    2021 2021-07-07 00:00:00        2.26                 15
    ##  3        3 July    2021 2021-07-07 00:00:00        1.62                 15
    ##  4        4 July    2021 2021-07-16 00:00:00        1.76                 15
    ##  5        5 July    2021 2021-07-30 00:00:00        1.53                 15
    ##  6        6 August  2021 2021-08-11 00:00:00        2.06                 15
    ##  7        7 August  2021 2021-08-14 00:00:00        1.9                  15
    ##  8        8 August  2021 2021-08-16 00:00:00        2.16                 15
    ##  9        9 August  2021 2021-08-16 00:00:00        2.6                  15
    ## 10       10 August  2021 2021-08-17 00:00:00        3.21                 15
    ## 11       11 August  2021 2021-08-17 00:00:00        2.44                 15
    ## 12       12 August  2021 2021-08-18 00:00:00        2.62                 15
    ## # ℹ 143 more rows
    ## # ℹ 6 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>

Now we try to combine the 3 datasets together. We observe that the 3
datasets have the same variables except that prof trashwheel lacks
“sports_balls” and Gwynnda trashwheel lacks “glass_bottles” and
“sports_balls”. We need to add them before combining.

``` r
prof_trashwheel_tidy_df = 
  prof_trashwheel_df %>% 
  mutate(sports_balls = NA) %>% 
  select(1:which(names(prof_trashwheel_df) == "wrappers"), sports_balls, everything()) 

print(prof_trashwheel_tidy_df, n = 12)
```

    ## # A tibble: 106 × 14
    ##    dumpster month     year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>    <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 January   2017 2017-01-02 00:00:00        1.79                 15
    ##  2        2 January   2017 2017-01-30 00:00:00        1.58                 15
    ##  3        3 February  2017 2017-02-26 00:00:00        2.32                 18
    ##  4        4 February  2017 2017-02-26 00:00:00        3.72                 15
    ##  5        5 February  2017 2017-02-28 00:00:00        1.45                 15
    ##  6        6 March     2017 2017-03-30 00:00:00        1.71                 15
    ##  7        7 April     2017 2017-04-01 00:00:00        1.82                 15
    ##  8        8 April     2017 2017-04-20 00:00:00        2.37                 15
    ##  9        9 May       2017 2017-05-10 00:00:00        2.64                 15
    ## 10       10 May       2017 2017-05-26 00:00:00        2.78                 15
    ## 11       11 June      2017 2017-06-20 00:00:00        2.34                 15
    ## 12       12 July      2017 2017-07-17 00:00:00        1.63                 15
    ## # ℹ 94 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <lgl>, homes_powered <dbl>

Similarly, we add “glass_bottles” and “sports_balls” to the Gwynnda
Trash Wheel dataset

``` r
gw_trashwheel_tidy_df = 
  gw_trashwheel_df %>% 
  mutate(glass_bottles = NA, sports_balls = NA) %>% 
  select(1:which(names(gw_trashwheel_df) == "wrappers"), sports_balls, everything()) %>% 
  select(1:which(names(gw_trashwheel_df) == "cigarette_butts"), glass_bottles, everything())
  

print(gw_trashwheel_tidy_df, n = 12)
```

    ## # A tibble: 155 × 14
    ##    dumpster month   year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 July    2021 2021-07-03 00:00:00        0.93                 15
    ##  2        2 July    2021 2021-07-07 00:00:00        2.26                 15
    ##  3        3 July    2021 2021-07-07 00:00:00        1.62                 15
    ##  4        4 July    2021 2021-07-16 00:00:00        1.76                 15
    ##  5        5 July    2021 2021-07-30 00:00:00        1.53                 15
    ##  6        6 August  2021 2021-08-11 00:00:00        2.06                 15
    ##  7        7 August  2021 2021-08-14 00:00:00        1.9                  15
    ##  8        8 August  2021 2021-08-16 00:00:00        2.16                 15
    ##  9        9 August  2021 2021-08-16 00:00:00        2.6                  15
    ## 10       10 August  2021 2021-08-17 00:00:00        3.21                 15
    ## 11       11 August  2021 2021-08-17 00:00:00        2.44                 15
    ## 12       12 August  2021 2021-08-18 00:00:00        2.62                 15
    ## # ℹ 143 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <lgl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <lgl>, homes_powered <dbl>

Now combine them.

``` r
all_trashwheels_df = bind_rows(mr_trashwheel_df, prof_trashwheel_tidy_df, gw_trashwheel_tidy_df)
```

This dataset (845 x 14) contains information from the Mr. Trashwheel
trash collectors in Baltimore, Maryland. As trash enters the inner
harbor, the trashwheels collect that trash, and store it in a dumpster.
The dataset contains information on year, month, and trash collected,
including specific kinds of trash. There are a total of 845 rows in our
final dataset.

- The total weight of trash collected by Professor Trash Wheel is 216.26
  tons.
- The total number of cigarette butts collected by Gwynnda in June of
  2022 is 18120.

``` r
sum(pull(prof_trashwheel_tidy_df, weight_tons))
```

    ## [1] 216.26

``` r
gw_trashwheel_tidy_df %>% 
  filter(year == 2022, month == "June") %>% 
  pull(cigarette_butts) %>% 
  sum()
```

    ## [1] 18120

# Problem 3

Import and clean the bakers dataset

``` r
bakers_df = 
  read_csv("./data/gbb/bakers.csv") %>% 
  janitor::clean_names() %>% 
  arrange(series) %>% 
  rename(baker = baker_name)
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
print(bakers_df, n=12)
```

    ## # A tibble: 120 × 5
    ##    baker              series baker_age baker_occupation                 hometown
    ##    <chr>               <dbl>     <dbl> <chr>                            <chr>   
    ##  1 Annetha Mills           1        30 Midwife                          Essex   
    ##  2 David Chambers          1        31 Entrepreneur                     Milton …
    ##  3 Edd Kimber              1        24 Debt collector for Yorkshire Ba… Bradford
    ##  4 Jasminder Randhawa      1        45 Assistant Credit Control Manager Birming…
    ##  5 Jonathan Shepherd       1        25 Research Analyst                 St Alba…
    ##  6 Lea Harris              1        51 Retired                          Midloth…
    ##  7 Louise Brimelow         1        44 Police Officer                   Manches…
    ##  8 Mark Whithers           1        48 Bus Driver                       South W…
    ##  9 Miranda Browne          1        37 Food buyer for Marks & Spencer   Midhurs…
    ## 10 Ruth Clemens            1        31 Retail manager/Housewife         Poynton…
    ## 11 Ben Frazer              2        31 Graphic Designer                 Northam…
    ## 12 Holly Bell              2        31 Advertising executive            Leicest…
    ## # ℹ 108 more rows

Import and clean the bakes dataset

``` r
bakes_df = 
  read_csv("./data/gbb/bakes.csv", na = c("N/A", "UNKNOWN", "")) %>% 
  janitor::clean_names()
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
print(bakes_df, n=12)
```

    ## # A tibble: 548 × 5
    ##    series episode baker     signature_bake                          show_stopper
    ##     <dbl>   <dbl> <chr>     <chr>                                   <chr>       
    ##  1      1       1 Annetha   "Light Jamaican Black Cakewith Strawbe… Red, White …
    ##  2      1       1 David     "Chocolate Orange Cake"                 Black Fores…
    ##  3      1       1 Edd       "Caramel Cinnamon and Banana Cake"      <NA>        
    ##  4      1       1 Jasminder "Fresh Mango and Passion Fruit Humming… <NA>        
    ##  5      1       1 Jonathan  "Carrot Cake with Lime and Cream Chees… Three Tiere…
    ##  6      1       1 Lea       "Cranberry and Pistachio Cakewith Oran… Raspberries…
    ##  7      1       1 Louise    "Carrot and Orange Cake"                Never Fail …
    ##  8      1       1 Mark      "Sticky Marmalade Tea Loaf"             Heart-shape…
    ##  9      1       1 Miranda   "Triple Layered Brownie Meringue Cake\… Three Tiere…
    ## 10      1       1 Ruth      "Three Tiered Lemon Drizzle Cakewith F… Classic Cho…
    ## 11      1       2 Annetha   "Rose Petal Shortbread"                 Pink Swirl …
    ## 12      1       2 David     "Cheddar Cheese and Fresh Rosemary Bis… Choux Pastr…
    ## # ℹ 536 more rows

Import and clean the results dataset.

``` r
results_df = 
  read_csv("./data/gbb/results.csv", skip = 2) %>% 
  janitor::clean_names()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
print(results_df, n=12)
```

    ## # A tibble: 1,136 × 5
    ##    series episode baker     technical result
    ##     <dbl>   <dbl> <chr>         <dbl> <chr> 
    ##  1      1       1 Annetha           2 IN    
    ##  2      1       1 David             3 IN    
    ##  3      1       1 Edd               1 IN    
    ##  4      1       1 Jasminder        NA IN    
    ##  5      1       1 Jonathan          9 IN    
    ##  6      1       1 Louise           NA IN    
    ##  7      1       1 Miranda           8 IN    
    ##  8      1       1 Ruth             NA IN    
    ##  9      1       1 Lea              10 OUT   
    ## 10      1       1 Mark             NA OUT   
    ## 11      1       2 David             8 IN    
    ## 12      1       2 Edd               6 IN    
    ## # ℹ 1,124 more rows

Now we want to create a big dataframe with all variables in one row:
series, episode, baker_name, baker_age, baker_occupation, hometown,
signature_bake, show_stopper, technical, result.

For the purpose of joining the datasets, I kept the first name of the
bakers. I joined the bakes dataset with the results dataset, using
“baker” “episode”, and “series” as the key. Then, I joined the dataset
with the baker dataset. The bakes dataset only contains information up
to the 8th season, so the bakes columns for the last 2 seasons are
filled with NAs.

``` r
bakers_fn_df = bakers_df %>% 
  mutate(baker = word(baker, 1))
```

``` r
gbb_df =
  bakes_df %>% 
  right_join(results_df, by = c("baker", "episode", "series")) %>% 
  left_join(bakers_fn_df, by = c("baker", "series")) %>% 
  write_csv("./data/gbb/gbb.csv")
```

The dataset (1136x10) contains data from the 1st to 10th season of the
Great British Bakeoff. Each season has 10 episodes, except the first two
seasons. In each episode, contestants compete in signature challenges,
technical challenges, and a showstopper. At the end of an episode the
winner is crowned “Star Baker” (and winner in the last episode of a
season), and a loser is eliminated. The dataset contains information
about the contestants, their signature bakes and showstoppers, and the
results. Each row is information about one contestant in one episode.

``` r
winner5t10_df = 
  gbb_df %>% 
  filter(series %in% c(5,6,7,8,9,10)) %>% 
  filter(result %in% c("WINNER", "STAR BAKER")) %>% 
  select(-signature_bake, -show_stopper) %>% 
  write_csv("./data/gbb/winner5t10.csv")
```

Here is compiled results of season 5-10. If we are trying to predict the
overall winner based on the star bakers of each episode, we are out of
luck. The contestant who is the star baker in most episodes tends not to
be the overall winner. The winner tends to come from someone who is only
the star baker in one or two episodes. In the most extreme case, like in
season 10, the winner is someone who is never crowned star baker. In
some seasons, the winner tends to be the star baker of the last few
episodes.

Finally, we will look at the viewers dataset.

``` r
viewers_df =
  read_csv("./data/gbb/viewers.csv", na = "NA") %>% 
  janitor::clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Mean viewership of season 1 is: 2.77 Mean viewership of season 5 is:
10.0393

``` r
select(viewers_df, series_1) %>% 
  drop_na() %>% 
  pull() %>% 
  mean()
```

    ## [1] 2.77

``` r
select(viewers_df, series_5) %>% 
  drop_na() %>% 
  pull() %>% 
  mean()
```

    ## [1] 10.0393
